\chapter{مفاهیم بنیادی}

\section{مقدمه}
در این فصل، مفاهیم بنیادی و پیش‌نیازهایی که برای درک بهتر پژوهش حاضر ضروری هستند معرفی می‌شوند. از آنجا که این پایان‌نامه در تقاطع علوم زیستی و روش‌های محاسباتی قرار دارد، آشنایی با مفاهیم هر دو حوزه برای دنبال‌کردن مطالب فصل‌های بعدی اهمیت ویژه‌ای دارد.
بر همین اساس، در بخش نخست این فصل، مفاهیم زیستی مرتبط با موضوع پژوهش مورد بررسی قرار می‌گیرند. سپس در بخش دوم، به معرفی مفاهیم محاسباتی مورد استفاده پرداخته می‌شود و تمرکز اصلی بر مباحث مرتبط با شبکه‌های عصبی گرافی و چارچوب‌های یادگیری مبتنی بر گراف خواهد بود.
در نهایت، در بخش پایانی این فصل، معیارهای ارزیابی به‌کاررفته در این پژوهش برای سنجش کیفیت شناسایی ماژول‌های عملکردی معرفی شده و به‌طور خلاصه تشریح می‌شوند تا زمینه لازم برای تحلیل نتایج در فصل‌های بعدی فراهم شود.

\section{مفاهیم زیستی}
در این پژوهش، یک روش محاسباتی به‌منظور شناسایی مجموعه‌های پروتئینی ارائه می‌شود. در این راستا، با برخی مفاهیم زیستی مرتبط مواجه می‌شویم که در این بخش، توضیحات مختصر و روشنی از هر یک با هدف تسهیل درک موضوع پژوهش ارائه شده است.
\subsection{پروتئین}
پروتئین‌ها از مهم‌ترین  درشت مولکول‌های زیستی در سلول‌های زنده به‌شمار می‌آیند که از تکرار واحدهای اسید آمینه ساخته شده‌اند و نقش‌های حیاتی و متنوعی را در ساختار و عملکرد سلول ایفا می‌کنند. این مولکول‌ها به‌طور ویژه به‌عنوان کارگزاران مولکولی سلول شناخته می‌شوند، زیرا در فرایندهایی مانند کاتالیز واکنش‌های شیمیایی (از طریق آنزیم‌ها)، انتقال مولکول‌ها، پیام‌رسانی درون‌سلولی، ایجاد حرکت و حفظ یکپارچگی ساختار سلولی نقش اساسی دارند. توالی اسیدهای آمینه هر پروتئین توسط ژن مربوطه تعیین شده و طی فرایند ترجمه از روی آران‌ای(ریبونوکلئیک اسید)\footnote{\lr{mRNA: messenger ribonucleic acid}} پیام‌رسان سنتز می‌شود. پس از سنتز، پروتئین‌ها از طریق فرایند تاخوردگی\footnote{Folding} به ساختارهای سه‌بعدی مشخصی دست می‌یابند که برای عملکرد زیستی آن‌ها ضروری است. ویژگی‌های عملکردی هر پروتئین به ترتیب خاص اسیدهای آمینه و برهم‌کنش‌های فضایی میان آن‌ها وابسته است. گستردگی و تنوع عملکردهای پروتئین‌ها به‌گونه‌ای است که تقریباً تمامی فرایندهای زیستی سلول، به‌صورت مستقیم یا غیرمستقیم، تحت تأثیر یا کنترل آن‌ها قرار دارند \cite{Alberts2022MBC}.

\subsection{ماژول‌های عملکردی}

فعالیت‌های زیستی در سلول و به‌طور کلی در بدن، معمولاً حاصل عملکرد یک پروتئین منفرد نیستند، بلکه نتیجه‌ی همکاری هماهنگ مجموعه‌ای از پروتئین‌ها می‌باشند که به‌صورت سازمان‌یافته با یکدیگر در ارتباط هستند. این پروتئین‌ها از طریق تعاملات مختلف، به‌ویژه تعاملات فیزیکی، در انجام یک یا چند وظیفه‌ی زیستی مشخص مشارکت می‌کنند \cite{adappi}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/functional-module.png}
	\label{fig:functional-module}
	\caption{ماژول‌های عملکردی }
\end{figure}

به چنین مجموعه‌ای از پروتئین‌ها که به‌صورت هماهنگ برای انجام یک عملکرد زیستی مشترک عمل می‌کنند، مجموعه‌ی پروتئینی یا ماژول عملکردی\footnote{\lr{Functional Module}} گفته می‌شود. هر ماژول عملکردی معمولاً بیانگر یک فرآیند زیستی، مسیر مولکولی یا سازوکار تنظیمی خاص در سلول است و اجزای آن، از نظر عملکردی به یکدیگر وابسته‌اند \cite{complexRefLocal}.

تعامل فیزیکی میان پروتئین‌ها که تحت عنوان تعامل پروتئین‌--پروتئین شناخته می‌شود، نقش محوری در شکل‌گیری و پایداری این ماژول‌های عملکردی ایفا می‌کند. این تعاملات امکان انتقال سیگنال، تنظیم فعالیت‌های آنزیمی و هماهنگی زمانی و مکانی پروتئین‌ها را فراهم می‌سازند و از این رو، برای درک صحیح بسیاری از فعالیت‌های زیستی ضروری هستند \cite{hartwell1999molecular}.

از جمله فرآیندهای زیستی مهمی که مبتنی بر ماژول‌های عملکردی هستند می‌توان به رونوشت دی‌اِن‌اِی، رونوشت آر‌اِن‌اِی پیام‌رسان و تنظیم چرخه‌ی سلولی اشاره کرد. در هر یک از این فرآیندها، گروه مشخصی از پروتئین‌ها به‌صورت شبکه‌ای از تعاملات عمل می‌کنند و اختلال در هر یک از اجزای این شبکه می‌تواند منجر به بروز نقص عملکردی در کل فرآیند شود.

در سال‌های اخیر، پیشرفت در شناسایی و تحلیل ماژول‌های عملکردی، به یکی از موضوعات مهم در زیست‌شناسی سامانه‌ای و بیوانفورماتیک تبدیل شده است. شناسایی دقیق این ماژول‌ها کاربردهای گسترده‌ای از جمله پیش‌بینی عملکرد پروتئین‌های ناشناخته \cite{li2012towards}، درک مکانیسم‌های مولکولی بیماری‌ها \cite{safari2014protein} و کشف اهداف دارویی جدید \cite{mujawar2019delineating} دارد. از این رو، مطالعه و مدل‌سازی ماژول‌های عملکردی نقش کلیدی در توسعه روش‌های نوین تشخیصی و درمانی ایفا می‌کند.


\subsection{بیان ژن\footnote{\lr{Gene expression}}}
بیان ژن  فرآیندی است که طی آن اطلاعات نهفته در توالی دی‌اِن‌اِی به محصولات عملکردی، عمدتاً آر‌اِن‌اِی و پروتئین، تبدیل می‌شود. این فرآیند شامل مراحل متعددی از جمله رونوشت دی‌اِن‌اِی به آر‌اِن‌اِی و در بسیاری از موارد ترجمه آراِن‌اِی ‌به پروتئین است و نقش اساسی در تعیین ساختار، عملکرد و رفتار سلول ایفا می‌کند. سطح بیان هر ژن نشان‌دهنده‌ی میزان فعالیت آن ژن در یک شرایط زیستی خاص بوده و به‌طور دقیق تحت تأثیر سازوکارهای تنظیمی مختلفی مانند عوامل رونویسی، تغییرات اپی‌ژنتیکی و سیگنال‌های درون‌سلولی و برون‌سلولی قرار دارد. تفاوت در الگوهای بیان ژن میان سلول‌ها، بافت‌ها یا شرایط فیزیولوژیک و پاتولوژیک مختلف، عامل اصلی تنوع عملکردی سلول‌ها محسوب می‌شود. از این رو، تحلیل داده‌های بیان ژن ابزار مهمی برای درک فرآیندهای زیستی، شناسایی مسیرهای مولکولی مختل‌شده در بیماری‌ها و استخراج نشانگرهای زیستی به‌شمار می‌رود \cite{gelard2024bulkrnabert}.

\subsection{پایگاه داده هستی شناسی ژن\footnote{\lr{Gene Ontology}}}
\lr{GO} یک بانک داده و سیستم طبقه‌بندی است که با هدف ایجاد یک زبان استاندارد برای توصیف ژن‌ها و محصولات ژنی (که پروتئین‌ها نیز جزو آنها هستند) ایجاد شده است.
این پروژه اطلاعات ساختاریافته و قابل پرداش از فرايند‌های زیستی، عملکرد مولکولی و مولفه‌ی سلولی ژن‌ها فراهم می‌کند.
داده‌های پروژه  \lr{GO} به صورت گسترده‌ای در تحقیقات مربوط به علوم زیستی مورد استفاده قرار می‌گیرد و همینطور همواره اطلاعات آن از نظر کمیت و کیفیت درحال تغییر است \cite{gene2019gene}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/go-attributes.png}
	\label{fig:go-attributes}
	\caption{ویژگی‌های عبارت \lr{GO:0016597}\cite{fa_wiki_gene_ontology}}
\end{figure}

هر عبارت \lr{GO} شامل موارد زیر می‌شود \cite{goOverview}:
\begin{itemize}
    \item یک نام که برای انسان قابل فهم باشد.
    \item یک شناساگر مختص آن عبارت که با پیشوند \lr{GO:} آغاز می‌شود.
    \item یک تعریف مختصر از مفاهیمی که توسط این عبارت \lr{GO} نمایش داده می‌شود.
    \item ارتباط آن با سایر عبارات \lr{GO} ؛ که در گراف \lr{GO} هر عبارت (به جز عبارات ریشه‌ای) فرزند یک عبارت \lr{GO} دیگر است.
\end{itemize}
اطلاعات موجود در بانک داده \lr{GO} به صورت ساختمان داده گرافی ذخیره شده‌اند. هر عبارت داری یک یا چند فرزند است که در نتیجه ساختار گراف \lr{GO}، یک گراف جهت‌دار بدون دور\footnote{Directed acyclic graph} است .

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/go-structure.png}
	\label{fig:go-structure}
	\caption{ساختار هستی‌شناسی ژن \cite{fa_wiki_gene_ontology}}
\end{figure}

گراف \lr{GO} شامل چهار نوع یال \lr{\texttt{is\_a}}، \lr{\texttt{part\_of}}، \lr{\texttt{regulates}} و \lr{\texttt{has\_part}} است که هر یک به‌ترتیب بیانگر رابطهٔ «نوعی از»، «جزئی از»، «نقش تنظیم‌کنندگی» و «دارا بودن جزء» میان مفاهیم مختلف در این هستی‌شناسی می‌باشند \cite{ashburner2000gene}.
این سیستم شامل سه زیرگراف جهت‌دار بدون دور اصلی است که هر یک از آنها  جنبه خاصی از عملکرد زیستی را توصیف می‌کنند:

\textbf{فرآیند زیستی\footnote{\lr{Biological process}}} : این بخش به فرآیند‌های زیستی اشاره دارد که ژن و یا پروتئین خاصی در آن نقش دارد.

\textbf{عملکرد مولکولی\footnote{\lr{Molecular function}}} : این بخش عملکرد دقیق مولکولی ژن یا پروتئین را توصیف می‌کند.

\textbf{مولفه‌ی سلولی\footnote{\lr{Cellular component}}} : این بخش به مکانی که ژن یا پروتئین در آن قرار دارد اشاره می‌کند.
از ویژگی‌های دیگر این بانک داده نمایش اطلاعات به صورت سازماندهی شده و سلسله مراتبی است که شامل شبکه‌های بدون دور می‌شود و ویژگی‌ها به این صورت مرتب شده‌اند \cite{gene}. 


\subsection{شبکه‌های \lr{PPI} و ویژگی‌های آنها }
یک شبکه \lr{PPI} معمولا به صورت یک گراف بدون جهت $G=(V,E)$ نشان داده می شود که $V$ و $E$ به ترتیب نمایان‌گر پروتئین ها و تعاملات بین آنها می باشند. وزن های روی یال ها را می توان برای توصیف ویژگی های شبکه \lr{PPI}، مانند ویژگی‌های توپولوژیکی یا عملکردی استفاده کرد. شبکه های \lr{PPI} سه ویژگی توپولوژیکی زیر را دارند:

\begin{itemize}
    \item توزیع بدون مقیاس\footnote{\lr{Scale-free distribution}} : $P(k)$   مفهوم توزیع درجه یعنی احتمال اینکه یک گره در یک شبکه دقیقا \lr{k} پیوند داشته باشد را نشان می دهد. یک  شبکه \lr{PPI} دارای توزیع درجه توانی $P(k)\sim\:k^{-\lambda}$ می باشد\cite{evolution} . این ویژگی به این معنی است که پروتئین‌های تعامل‌دار در شبکه‌های \lr{PPI} به طور یکنواخت توزیع نمی شوند‌، بیشتر پروتئین‌ها تنها در چند تعامل شرکت می‌کنند در حالی که مجموعه کوچکی از پروتئین‌ها در ده‌ها تعامل (تشکیل گره هاب\footnote{\lr{Hub}}) شرکت می‌کنند. 
    \item ویژگی جهان کوچک\footnote{\lr{Small-world property}} : پروتئین‌های یک شبکه \lr{PPI} دارای میانگین طول مسیر کم و ضرایب خوشه‌ای بالا هستند\cite{protein}  که سیگنال‌های هر گره در شبکه \lr{PPI} را قادر می‌سازد تا از طریق چند جهش به سرعت به هر گره دیگری برسند. در نتیجه شبکه‌های \lr{PPI} هم زمان انتقال سیگنال و هم زمان پاسخ کوتاهی خواهند داشت.
    \item شبکه با ماژول‌های عملکردی\footnote{\lr{Functional modular network}} : شبکه \lr{PPI} یک شبکه ماژولار و سلسله مراتبی می‌باشد. یک ماژول عملکردی در یک شبکه \lr{PPI} یک مجموعه با بیشترین تعداد پروتئین که عملکرد یکسانی دارند، می‌باشد. بارزترین مشخصه ماژول عملکردی، ارتباط بین ساختار توپولوژیکی شبکه \lr{PPI} و عملکرد‌ پروتئین‌های آن است که مبنای بسیاری از روش‌های تشخیص ماژول عملکردی است\cite{molecular} \cite{road}.
\end{itemize}


\section{مفاهیم محاسباتی}

در این بخش، مفاهیم محاسباتی مورد استفاده در این پایان‌نامه معرفی می‌شوند. از آن‌جا که روش پیشنهادی این پژوهش بر پایه تحلیل شبکه‌ها و یادگیری عمیق استوار است، آشنایی با مبانی نظری مرتبط با گراف‌ها، الگوریتم‌های یادگیری عمیق و شبکه‌های عصبی گرافی برای درک بهتر مراحل روش ارائه‌شده ضروری است. بدین منظور، در ادامه مروری اجمالی بر مفاهیم و تعاریف اصلی ارائه می‌شود تا چارچوب محاسباتی پژوهش به‌صورت منسجم و شفاف تبیین گردد.

\subsection{یادگیری ماشین}
یادگیری ماشین\footnote{\lr{Machine Learning}} یکی از شاخه‌های هوش مصنوعی\footnote{\lr{Artificial Intelligence}} است که به رایانه‌ها امکان می‌دهد بدون تعریف صریح قوانین، الگوها و روابط موجود در داده‌ها را شناسایی کرده و بر اساس آن‌ها به پیش‌بینی یا تصمیم‌گیری بپردازند. در این رویکرد، مدل‌ها با استفاده از داده‌های آموزشی، دانش لازم را استخراج کرده و قادر خواهند بود این دانش را به داده‌های جدید تعمیم دهند. امروزه یادگیری ماشین در بسیاری از حوزه‌های پژوهشی و صنعتی مورد استفاده قرار می‌گیرد.

به‌طور کلی، روش‌های یادگیری ماشین به سه دسته‌ی اصلی یادگیری نظارت‌شده\footnote{\lr{Supervised Learning}}، یادگیری بدون نظارت \footnote{\lr{Unsupervised Learning}}و یادگیری تقویتی\footnote{\lr{Reinforcement Learning}} تقسیم می‌شوند که هر یک متناسب با نوع داده‌ها و هدف مسئله به‌کار گرفته می‌شوند \cite{islam2024comprehensive}.
\subsection{یادگیری عمیق}
یادگیری عمیق را می‌توان یکی از زیرشاخه‌های یادگیری ماشین دانست که بر پایه‌ی مدل‌هایی با چندین لایه‌ی پردازشی بنا شده است. این مدل‌ها معمولاً از لایه‌های متصل به‌هم یا لایه‌های پیچیده‌تر تشکیل می‌شوند و به‌دلیل ساختار چندلایه‌ی خود، دارای تعداد پارامترهای قابل یادگیری بیشتری نسبت به الگوریتم‌های کلاسیک یادگیری ماشین هستند. همین ویژگی موجب می‌شود که یادگیری عمیق توانایی بالاتری در مدل‌سازی روابط پیچیده میان داده‌ها داشته باشد.

تفاوت اصلی یادگیری عمیق با روش‌های سنتی یادگیری ماشین در نحوه‌ی استخراج ویژگی‌ها است. در الگوریتم‌های کلاسیک، ویژگی‌ها معمولاً به‌صورت دستی و با دخالت متخصص تعیین می‌شوند، در حالی که روش‌های یادگیری عمیق قادرند با توجه به داده‌های موجود و ماهیت مسئله، ویژگی‌های مناسب را به‌صورت خودکار استخراج کنند. این قابلیت باعث شده است که یادگیری عمیق در مسائل پیچیده و داده‌محور عملکرد بهتری از خود نشان دهد \cite{janiesch2021machine}.
\subsection{یادگیری نظارت شده}
یادگیری نظارت‌شده یکی از رایج‌ترین انواع یادگیری ماشین است که در آن الگوریتم با استفاده از داده‌های ورودی به‌همراه برچسب \footnote{Label} متناظر با هر نمونه آموزش می‌بیند. هدف از این فرایند، ساخت مدلی است که بتواند رابطه‌ میان داده‌ها و برچسب‌ها را آموخته و برای داده‌های جدید و دیده ‌نشده عملکرد مناسبی داشته باشد. در این نوع یادگیری، داده‌ها معمولاً به دو مجموعه‌ی آموزشی\footnote{\lr{Train dataset}} و آزمایشی\footnote{\lr{Test dataset}} تقسیم می‌شوند؛ بدین‌صورت که مدل با استفاده از داده‌های آموزشی ساخته شده و سپس با داده‌های آزمایشی مورد ارزیابی قرار می‌گیرد. به بیان ساده، وجود برچسب در کنار هر داده، ویژگی اصلی یادگیری نظارت‌شده محسوب می‌شود \cite{dridi2021supervised}.
		
\subsection{یادگیری بدون نظارت}
یادگیری بدون نظارت نوعی از یادگیری ماشین است که در آن الگوریتم‌ها با داده‌های بدون برچسب سروکار دارند و هدف اصلی آن‌ها کشف الگوها و ساختارهای پنهان موجود در داده‌ها است. برخلاف یادگیری نظارت‌شده، در این روش اطلاعاتی از خروجی مطلوب در اختیار مدل قرار نمی‌گیرد و الگوریتم تلاش می‌کند بر اساس شباهت‌ها و ویژگی‌های ذاتی داده‌ها، آن‌ها را سازمان‌دهی کند. از شناخته‌شده‌ترین الگوریتم‌های یادگیری بدون نظارت می‌توان به روش‌های خوشه‌بندی اشاره کرد که با هدف قرار دادن داده‌های مشابه در خوشه‌های یکسان به‌کار می‌روند \cite{wu2021review}.
\subsection{گراف}
یک گراف از مجموعه‌ای غیر خالی از اشیا به نام راس تشکیل شده، که آن را با $V$ نشان می‌دهیم، و مجموعه‌ای شامل یال‌ها، که راس‌ها را به هم وصل می‌کنند و با $E$ نمایش می‌دهیم. یک چنین گرافی را با $G=(V,E)$ نشان می‌دهیم. اگر یال $e$ دو راس $v_1$ و $v_2$ را به هم وصل کند می‌نویسیم $e = \{ v_1, v_2 \}$ \cite{Babalian2007DiscreteMath}. تعریف ارائه شده، تعریف گراف ساده است. اما انواع مختلفی از گراف موجود می‌باشد که در ادامه به بررسی دو نوع از آن‌ها (گراف جهت‌دار و گراف وزن‌دار) می‌پردازیم:
\begin{itemize}
	\item \textbf{گراف جهت‌دار}:
	گراف $G(V,E)$ زمانی جهت‌دار است که مجموعه $E$، از جفت اعضایی همانند $(u,v) ;\; u,v \in V$ تشکیل شده باشد و ترتیب این دوتایی‌ها نشان‌دهنده جهت یال مربوطه باشد. به این صورت برای هر یال جهت نیز درنظر گرفته می‌شود که به گراف حاصل، گراف جهت‌دار می‌گوییم.
	\item \textbf{گراف وزن‌دار}:
	گراف $G(V,E,W)$ که $W \in R^{|E|}$ یک مقدار عددی به هر یک از یال‌ها اختصاص می‌دهد که میزان وزن آن یال است.
	
\end{itemize}

\subsection{شبکه‌های عصبی گرافی}
شبکه‌های عصبی گرافی\footnote{\lr{Graph neural networks - GNNs}} اولین بار در سال ۲۰۰۵ پیشنهاد شدند. شبکه‌های عصبی گرافی، دسته‌ای از شبکه‌های عصبی هستند که برای مدیریت داده‌های سازمان‌دهی شده در ساختارهای گراف طراحی شده‌اند. 
شبکه‌های عصبی گرافی بر پایه مکانیسم انتقال پیام\footnote{\lr{Message passing}} هستند.
\begin{figure}[h]
	\includegraphics[width=\textwidth]{figures/message-passing.png}
	\label{fig:message-passing}
	\caption{شماتیک مکانیزم انتقال پیام \cite{grattarola2021gnn2}}
\end{figure}

در ابتدا یک گراف با ماتریس ویژگی گره‌ها  $X \in R^{|V| \times d}$ به عنوان ورودی در نظر گرفته می‌شود که $|v|$ تعداد گره‌های گراف و $d$ بعد ویژگی‌های گراف می‌باشد. در شبکه‌های عصبی گرافی از این ویژگی‌ها در کنار ساختار گراف برای تولید بازنمایی‌های هر گره استفاده می‌شود. در هر تکرار، هر گره اطلاعاتی را از گره‌های همسایگی خود جمع‌آوری می‌کند که این عمل را به صورت کلی جمع‌آوری\footnote{Aggregate} می‌نامند. در مرحله بعد شبکه باید اطلاعات جمع‌آوری شده را با اطلاعات موجود گره ادغام کند و بازنمایی جدیدی از گره مورد نظر ارائه دهد. به صورت کلی این مرحله از انتقال پیام را نیز بروزرسانی\footnote{Update} می‌نامند. به طورخلاصه در یکبار انتقال پیام مراحل زیر طی می‌شوند:
\begin{equation}
h_u^{(k+1)} = UPDATE^{(k)}(h_u^{(k)}, AGGREGATE(\{h_v^{(k)}, \forall_v \in N(u)\}))
\label{eq:message-passing}
\end{equation}

در فرمول \ref{eq:message-passing}  نمادهای \lr{AGGREGATE} و \lr{UPDATE}، دو تابع دلخواه مشتق‌پذیر (به عنوان مثال یک شبکه عصبی) هستند و $N(u)$ نشانگر مجموعه همسایگان گره $u$ است. همچنین $h_u^{(k)}$ نشان‌دهنده بازنمایی گره $u$ در مرحله $k$ام است. 

با افزایش این مراحل، بازنمایی هر گره داده‌های بیشتری از گره‌های دورتر از خود در گراف خواهد داشت. پس از اولین تکرار $( \; k = 1 \;)$, هر بازنمایی گره اطلاعات مربوز به همسایگی تک گامی خود را حفظ می‌کند، که ممکن است در گراف از طریق مسیری به طول ۱ قابل دسترسی باشد\cite{rong2019dropedge}. بعد از دومین تکرار $( \; k = 2 \;)$, بازنمایی هر گره شامل اطلاعاتی از همسایگی با دو گام است؛ به طور کلی، پس از $k$ مرحله، بازنمایی هر گره می‌تواند شامل داده‌هایی از گره‌هایی با فاصله $k-hop$ از خود باشد. براساس مکانیزم انتقال پیام در شبکه‌های عصبی گرافی، این شبکه‌ها در تولید بازنمایی‌هایی که هم اطلاعات مربوط به ساختار گراف و هم ویژگی‌های گره‌ها را حفظ کنند بسیار موفق بوده‌اند. و به همین دلیل در بسیاری از مسائل مورد استفاده قرار گرفته‌اند. بنابر توابع استفاده شده به عنوان تابع جمع آوری و بروزرسانی، شبکه‌های عصبی گرافی به انواع مختلفی همانند شبکه‌های عصبی گرافی پیچشی، شبکه‌های عصبی گرافی توجه محور و دیگر دسته‌ها تقسیم بندی می‌شوند \cite{khemani2024review}.




\subsection{شبکه‌های عصبی گرافی پیچشی}
پژوهش کیف و ولینگ \cite{kipf2016semi} با هدف ارائه مدلی ساده، مقیاس‌پذیر و قابل‌اجرا برای یادگیری روی گراف‌های بزرگ، شبکه‌های عصبی گرافی پیچشی را معرفی کرد. روش‌های طیفی پیشین مبتنی بر تجزیه‌ی لاپلاسین گراف بوده و نیازمند محاسبه‌ی مفادیر ویژه و بردارهای ویژه‌ بودند که این امر هزینه‌ی محاسباتی بالایی داشته و استفاده از آن‌ها را در گراف‌های بزرگ محدود می‌کرد. شبکه گرافی پیشنهاد شده از معادله \ref{eq:gcn-propagation} برای انتقال پیام استفاده می‌کند.
\begin{equation}
	H^{(k+1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}H^{k}W^{k})
	\label{eq:gcn-propagation}
\end{equation}
در اینجا، $\tilde{A} = A + I_N$ ماتریس مجاورت گراف بدون‌جهت $G$ با در نظر گرفتن یال‌های خودی\footnote{\lr{Self-connections}} است. 
$I_N$ ماتریس همانی بوده و $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$ عناصر قطری ماتریس درجه متناظر را تشکیل می‌دهد. 
همچنین، $W^{(k)}$ یک ماتریس وزن قابل‌آموزش مخصوص لایه $k$ام است. 
تابع $\sigma(\cdot)$ نشان‌دهنده یک تابع فعال‌سازی است که برای مثال می‌تواند تابع \lr{ReLU} به‌صورت 
$\mathrm{ReLU}(\cdot) = \max(0,\cdot)$ باشد. 
ماتریس $H^{(k)} \in \mathbb{R}^{N \times D}$ نمایش‌دهنده فعال‌سازی‌ها در لایه $k$ام بوده و 
$H^{(0)} = X$ به‌عنوان ورودی اولیه شبکه در نظر گرفته می‌شود.
با توجه به تعریف ارائه شده در بخش شبکه‌های عصبی گرافی و مکانیزم انتقال پیام، توابع \lr{AGGREGATE} و \lr{UPDATE}، به ترتیب از معادله‌های
$S=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2} H^{(k)}}$
و 
$\sigma(SW^{(k)})$
 پیروی می‌کنند.
 
 با وجود سادگی و کارایی بالا، شبکه‌های عصبی گرافی پیچشی دارای محدودیت‌هایی نیز هستند. از جمله این محدودیت‌ها می‌توان به پدیده‌ی هموارسازی بیش‌ازحد\footnote{\lr{Over-smoothing}} در صورت افزایش تعداد لایه‌ها اشاره کرد که در آن نمایش گره‌ها به مرور شباهت زیادی به یکدیگر پیدا می‌کنند و توان تفکیک مدل کاهش می‌یابد. علاوه بر‌این، در این شبکه‌ها، تمامی گره‌های همسایه به یک اندازه در تشکیل بازنمایی جدید نقش دارند و این در حالی است که در بسیاری از گراف‌ها میزان اهمیت تمامی گره‌ها یکسان نیست. این محدودیت‌ها انگیزه‌ای برای توسعه‌ی مدل‌های پیشرفته‌تر شبکه‌های عصبی گرافی در پژوهش‌های بعدی بوده است.  

\subsection{شبکه‌های عصبی گرافی توجه محور}
ولیکویک و همکاران \cite{velivckovic2017graph}، با هدف رفع محدودیت‌های شبکه‌های عصبی گرافی پیچشی در تخصیص وزن یکسان به تمامی همسایه‌ها، مدل شبکه‌های عصبی گرافی با مکانیزم توجه\footnote{\lr{Graph Attention Network — GAT}} را معرفی کردند. این مدل امکان یادگیری وزن‌های متفاوت برای هر یال همسایگی را فراهم می‌کند و بدین ترتیب اهمیت نسبی گره‌های همسایه در تشکیل بازنمایی هر گره به‌ صورت داده‌محور تعیین می‌شود. این ویژگی باعث می‌شود \lr{GAT} توانایی مدل‌سازی پیچیدگی‌های ساختاری گراف‌هایی با ارتباطات غیرهمگن را داشته باشد، در حالی که \lr{GCN} تمامی همسایه‌ها را با وزن یکسان در نظر می‌گیرد.

در \lr{GAT}، انتقال پیام با استفاده از مکانیسم توجه خود-توجهی\footnote{\lr{Self-attention}} در معادله \ref{eq:gat-representation} تعریف می‌شود.

\begin{equation}
	h^{(k+1)}_i = \sigma\Big(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} W h^{(k)}_j \Big)
	\label{eq:gat-representation}
\end{equation}

که در آن، $h_i^{(k+1)}$ نمایش به‌روزشده گره $i$، $h^{(k)}_j$ ویژگی‌های گره همسایه $j$، $W$ ماتریس وزن قابل‌آموزش و $\sigma(\cdot)$ تابع فعال‌سازی است. ضریب توجه $\alpha_{ij}$ اهمیت گره $j$ را نسبت به گره $i$ مشخص می‌کند و با استفاده از یک شبکه کوچک خطی و تابع \lr{LeakyReLU} مطابق معادله \ref{eq:gat-attention} محاسبه می‌شود.

\begin{equation}
	\alpha_{ij} = \frac{\exp(\mathrm{LeakyReLU}(a^T [Wh^{(k)}_i | Wh^{(k)}_j]))}{\sum_{n \in \mathcal{N}(i)} \exp(\mathrm{LeakyReLU}(a^T [Wh^{(k)}_i | Wh^{(k)}_n]))}
	\label{eq:gat-attention}
\end{equation}

که در آن $a$ بردار وزن قابل‌آموزش برای مکانیزم توجه و $|$ عملگر ادغام ویژگی‌های گره‌ها است. به این ترتیب، تابع \lr{AGGREGATE} در \lr{GAT} به صورت جمع‌وزنی گره‌های همسایه با ضرایب توجه بوده و تابع \lr{UPDATE} شامل اعمال ماتریس وزن و تابع فعال‌سازی روی مقدار جمع‌وزنی شده است.

یکی از نوآوری‌های مهم \lr{GAT} استفاده از توجه چندسر\footnote{\lr{Multi-head attention}} است. در این روش، $K$ مکانیسم توجه مستقل بر روی همان لایه اعمال می‌شود و نتایج هر سر یا با هم ادغام می‌شوند (الحاق\footnote{\lr{Concat}} یا میانگین‌گیری) تا نمایی غنی‌تر و پایدارتر از ویژگی‌های گره‌ها تولید شود که در معادله \ref{eq:gat-multi} نمایش داده شده است.

\begin{equation}
	h_i^{(k+1)} = \big\Vert_{m=1}^M \sigma\Big(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^{(m)} W^{(m)} h_j^{(k)} \Big)
	\label{eq:gat-multi}
\end{equation}

استفاده از مکانیزم توجه چند سر، علاوه بر افزایش ظرفیت مدل، باعث کاهش حساسیت شبکه به نویز و نوسانات محلی گراف می‌شود و کمک می‌کند تا مدل بتواند اطلاعات مفیدی از همسایگان مختلف در سطوح گوناگون استخراج کند.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/GAT.jpg}
	\label{fig:GAT}
	\caption{مکانیزم توجه در شبکه‌های عصبی گرافی \cite{velivckovic2017graph}}
\end{figure}

با وجود مزایای قابل‌توجه، \lr{GAT} نیز محدودیت‌هایی دارد. پیچیدگی محاسباتی آن نسبت به \lr{GCN} بالاتر است، به ویژه در گراف‌های بزرگ با درجه بالا. همچنین، انتخاب تعداد سرهای توجه و تنظیمات مربوط به آنها می‌تواند تاثیر زیادی بر عملکرد شبکه داشته باشد و نیازمند تنظیمات دقیق است. با این حال، \lr{GAT} به‌خوبی امکان مدل‌سازی اهمیت متفاوت همسایگان، کاهش اثرات هموارسازی بیش‌ازحد و استخراج ویژگی‌های غیرهمگن را فراهم می‌کند و به همین دلیل در بسیاری از مسائل یادگیری روی گراف، نتایج بهتری نسبت به \lr{GCN} ارائه می‌دهد.

\subsection{شبکه‌های عصبی گرافی با انتقال دانش بین لایه‌ها}
شبکه‌های عصبی گرافی با انتقال دانش بین لایه‌ها\footnote{\lr{Jumping Knowledge Graph Neural Networks - JKNets}} برای نخستین بار در پژوهش ژو و همکاران \cite{xu2018representation} معرفی شدند. این پژوهش به یکی از چالش‌های اساسی در شبکه‌های عصبی گرافی اشاره می‌کند که ناشی از استفاده از بازنمایی‌های مبتنی بر فاصله‌های ثابت همسایگی برای تمام گره‌ها است. در چنین رویکردی، تمامی گره‌ها صرف‌نظر از موقعیت ساختاری خود در گراف، با تعداد یکسانی از لایه‌ها پردازش می‌شوند؛ در حالی که ساختار زیرگرافی گره‌ها می‌تواند به‌طور قابل‌توجهی با یکدیگر متفاوت باشد.

نویسندگان این پژوهش بر این باورند که به‌منظور استخراج بازنمایی مناسب‌تر برای هر گره، لازم است مرتبه‌ی بازنمایی آن متناسب با ویژگی‌های ساختاری همان گره انتخاب شود. به بیان دیگر، برخی گره‌ها برای دستیابی به بازنمایی معنادار به اطلاعات محلی نیاز دارند، در حالی که برای برخی دیگر، بهره‌گیری از همسایگی‌های دورتر ضروری است.

برای نمونه، در پژوهش کیف و همکاران \cite{kipf2016semi} که منجر به معرفی شبکه‌های عصبی گرافی پیچشی شد، نتایج تجربی نشان می‌دهد که استفاده از دو لایه بهترین عملکرد را به همراه دارد. این در حالی است که از منظر تئوری، افزایش تعداد لایه‌ها باید امکان تجمیع اطلاعات گسترده‌تر و یادگیری بازنمایی‌های غنی‌تر را فراهم کند. این ناسازگاری بیانگر وجود محدودیت‌هایی در تعمیق شبکه‌های عصبی گرافی است.

در حوزه‌ی بینایی ماشین، مشکل تعمیق شبکه‌ها با بهره‌گیری از سازوکار اتصال باقیمانده\footnote{\lr{Residual Connection}} تا حد زیادی برطرف شده است \cite{he2016deep}. با این حال، در شبکه‌های عصبی گرافی، حتی با وجود استفاده از اتصال‌های باقیمانده، مسئله‌ی همگن‌شدن بیش‌ازحد بازنمایی گره‌ها و محدودیت شعاع همسایگی همچنان باقی می‌ماند.

در همین راستا، ژو و همکاران با هدف رفع مشکل شعاع ثابت همسایگی و افزایش انعطاف‌پذیری در استخراج بازنمایی گره‌ها، سازوکار انتقال دانش بین لایه‌ها را پیشنهاد کردند. این سازوکار امکان ترکیب تطبیقی اطلاعات حاصل از لایه‌های مختلف شبکه را برای هر گره فراهم می‌کند و بدین ترتیب، هر گره می‌تواند از سطح مناسبی از اطلاعات محلی یا سراسری بهره‌مند شود.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/JKNet.png}
	\label{fig:JKNet}
	\caption{شماتیک پرش دانش در شبکه‌های عصبی گرافی \cite{xu2018representation}}
\end{figure}

مکانیزم انتقال دانش بین لایه‌ها مستقل از نوع معماری بوده و قابلیت اعمال بر روی انواع شبکه‌های عصبی گرافی را دارد. با در نظر گرفتن بازنمایی‌های استخراج‌شده از لایه‌های مختلف شبکه برای گره \( v \) به‌صورت
$
h_v^{(1)}, h_v^{(2)}, \ldots, h_v^{(k)}
$
، این سازوکار اقدام به ترکیب این بازنمایی‌ها با استفاده از روش‌های مختلفی نظیر بیشینه‌گیری\footnote{\lr{Max Pooling}}، الحاق\footnote{\lr{Concatenation}}، یا به‌کارگیری شبکه‌های حافظه‌ی کوتاه‌مدت بلندمدت مبتنی بر سازوکار توجه\footnote{\lr{LSTM-attention}} می‌کند. بدین ترتیب، بازنمایی نهایی هر گره به‌صورت تطبیقی و متناسب با ساختار آن در گراف شکل می‌گیرد.




\subsection{تعبیه گره‌ها به روش \lr{Node2Vec}}
روش \lr{Node2Vec} یک الگوریتم مقیاس پذیر\footnote{Scalable} نیمه نظارتی\footnote{\lr{Semi-Supervised}} برای یادگیری ویژگی‌ها از روی گراف است. این الگوریتم به طور مستقیم از الگوریتم یادگیری بازنمایی کلمات \lr{Word2Vec}\cite{mikolov2013efficient} که در زمینه پردازش زبان طبیعی\footnote{\lr{Natural Language Processing - NLP}} استفاده می‌شود، ایده گرفته است. در این روش هدف تابع بهینه‌سازی، بیشینه کردن احتمال مشاهده گره‌های همسایه یک گره به شرط مشاهده خود آن گره است. هدف نهایی این الگوریتم یادگیری یک بازنمایی $d$ بعدی برای هر گره است \cite{grover2016node2vec}. این روش در ابتدا اقدام به جایگشت تصادفی بر روی گراف به کمک الگوریتم‌های نمونه برداری اول سطح\footnote{\lr{Breadth-first sampling - BFS}} و اول عمق\footnote{\lr{Depth-first sampling - DFS}} می‌کند. انتخاب روش مناسب نمونه‌برداری از اهمیت بالایی برخوردار است. در نمونه‌برداری اول سطح، هدف اصلی استخراج بازنمایی‌های مشابه برای گره‌هایی است که از قوانین ساختاری یکسانی پیروی می‌کنند؛ در حالی‌که نمونه‌برداری اول عمق بر ایجاد بازنمایی‌های مشابه برای گره‌هایی تمرکز دارد که به‌صورت چگال به یکدیگر متصل هستند. در عمل، بهترین راهکار استفاده از یک روش ترکیبی است؛ به‌گونه‌ای که بخشی از توالی‌ها با استفاده از نمونه‌برداری اول عمق و بخش دیگری با استفاده از نمونه‌برداری اول سطح تولید شوند. سپس یک شبکه عصبی آموزش داده می‌شود تا با استفاده از مشاهده مسیرهای پیشین، گره بعدی را پیش‌بینی کند (مشابه فرآیند آموزش در روش \lr{Word2Vec}). بدین منظور، تابع هزینه\footnote{\lr{Loss function}} مطابق با معادله \ref{eq:node2vec-loss} تعریف می‌شود.
	\begin{equation}
	\max_f \sum_{u \in V} \log Pr(N_S(u) \mid f(u))
	\label{eq:node2vec-loss}
	\end{equation}
\subsection{خوشه بندی در گراف های با گره های دارای ویژگی }
با فرض گراف $G=(V,E,F)$ که در آن $V$ مجموعه گره ها، $E$ مجموعه یال ها است و $F$ ماتریس ویژگی های گره ها می باشد، یک خوشه بندی از گراف $G$ را می توان با $C$ نشان داد که مجموعه ای از زیر مجموعه های $V$ است، به صورتی که  $C_i\in C\:;\:C_i\subset V$. هدف از خوشه بندی این است که خوشه هایی که هم از نظر ساختاری و هم از نظر ویژگی های گره‌ها بهم بیشترین شباهت را دارند، پیدا کنیم. همچنین خوشه‌های ایجاد شده باید از نظر ارتباط‌ یال‌های داخل خوشه چگال و در ارتباط یال‌ها با دیگر خوشه‌ها تنک باشند. نکته مهم دیگر در این قسمت وجود و یا عدم وجود همپوشانی در بین خوشه‌ها می‌باشد که به خوشه بندی بدون همپوشانی، افرازبندی\footnote{Partitioning} نیز می‌گویند. به عبارت دیگر در افرازبندی شرط:
$$ 
\forall i,j;\; i\ne j;\; C_i \in C \;\; and \;\; C_j \in C;\; C_i \cap C_j \subseteq \phi
$$
باید حتما رعایت شود این در حالیست که در خوشه‌بندی با همپوشانی چنین شرطی الزامی نیست.
\subsection{دسته‌بندی و روش‌های کلی خوشه‌بندی گراف}
 روش‌های خوشه‌بندی گراف را می‌توان از دیدگاه‌های مختلفی تقسیم‌بندی کرد. این تقسیم‌بندی‌ها بر اساس معیارها و ویژگی‌های خاصی صورت می‌گیرند که به نحوه برخورد با داده‌های گرافی، نوع اطلاعات استفاده شده، و تکنیک‌های به کار گرفته شده بستگی دارد. در این پژوهش از آنجایی که نوع گراف ورودی مشخص است و قصد خوشه‌بندی گراف‌های \lr{PPI} با گره‌های دارای ویژگی را داریم، روش‌های خوشه‌ بندی را بر اساس روش‌ مورد استفاده تقسیم‌بندی می‌کنیم:

\begin{itemize}
    \item روش‌های طیفی\footnote{\lr{Spectral clustering}} : از مقادیر ویژه\footnote{\lr{Eigenvalues }}  ماتریس لاپلاسین یا مجاورت برای یافتن خوشه‌ها استفاده می‌کنند.
    \item روش‌های فاکتورگیری ماتریسی\footnote{\lr{Matrix factorization}} : از روش‌های تجزیه‌ ماتریسی مانند تجزیه نامنفی ماتریس\footnote{\lr{Non-negative matrix factorization}}  یا تجزیه مقدار تکین\footnote{\lr{Singular value factorization}}  برای ایجاد امبدینگ و خوشه‌بندی استفاده می‌کنند.
    \item روش‌های سلسله‌مراتبی\footnote{\lr{Hierarchical clustering}} : گراف را به صورت سلسله ‌مراتبی خوشه‌بندی می‌کنند که به دو روش تقسیمی و تجمعی دسته‌بندی می‌شوند.
    \item روش‌های مبتنی بر امبدینگ\footnote{\lr{Embedding-based methods}} : ابتدا گره‌ها به فضای برداری کم‌بعد نگاشت می‌شوند و سپس خوشه‌بندی روی این فضای برداری انجام می‌شود و تمرکز اصلی در این روش‌ها یافتن بازنمایی مناسب برای خوشه‌بندی گراف است. \lr{(Node2Vec, DeepWalk, GCN, GNN)}.
    \item روش‌های بدون امبدینگ\footnote{\lr{Non-embedding methods}} : مستقیماً از ساختار گراف برای خوشه‌بندی استفاده می‌شود بدون اینکه گره‌ها به فضای برداری منتقل شوند \lr{(Louvain, graph - cut based)}.

\end{itemize}

\section{معیار‌های ارزیابی}
در این قسمت به بررسی معیار‌های ارزیابی عملکرد الگوریتم‌های شناسایی مجموعه‌های پروتئینی می‌پردازیم. در بین معیار‌های موجود، معیارهای دقت\footnote{\lr{Precision}}، بازیابی\footnote{\lr{Recall}}، صحت\footnote{\lr{Accuracy}}، امتیاز F ، بیشترین استفاده را در بین پژوهش‌ها داشته‌اند که ما نیز به منظور تحلیل و مقایسه عملکرد روش خود از آنها استفاده می‌کنیم.
در ابتدا برای شروع به معیار شباهت همسایگی که برای محاسبه تمامی معیار‌های مذکور مورد نیاز است، می‌پردازیم:
\subsection{شباهت همسایگی\footnote{\lr{Neighborhood affinity}}}

با در نظر گرفتن $P$ به عنوان مجموعه‌ای از مجموعه‌های پروتئینی شناسایی شده توسط الگوریتم، عملکرد الگوریتم به وسیله تعداد مجموعه‌های پروتئینی مشترک بین $P$ و مجموعه‌ای از مجموعه پروتئین‌های مرجع\footnote{\lr{Reference protein complex}} $B$ بدست می‌آید. برای مشخص کردن اینکه آیا یک مجموعه پروتئین شناسایی شده $p \in P$ با یک مجموعه پروتئین مرجع $b \in B$ یکسان هستند یا خیر ما اقدام به محاسبه معیار شباهت همسایگی به صورت مقابل می‌کنیم:
\begin{equation}
NA(p,b) = \frac{|V_p \cap V_b|^2}{|V_p| \times |V_b|}
\end{equation}

که $V_p$ مجموعه پروتئین‌های حاضر در ترکیب $p$ و به طور مشابه $V_b$ مجموعه پروتئین‌های حاضر در $b$ هستند. برای تفسیر شباهت همسایگی یک آستانه\footnote{\lr{Threshold}} از قبل تعیین شده (معمولا 25/0) در نظر گرفته می‌شود که شباهت همسایگی‌های بالاتر از آستانه به معنی یکسانی دو مجموعه است. همچنین تعداد مجموعه‌های شناسایی شده‌ای که حداقل با یک مجموعه مرجع یکسان در نظر گرفته می‌شوند را با $N_{cp}$ و تعداد مجموعه های مرجعی که حداقل با یکی از مجموعه‌های شناسایی شده الگوریتمی یکسان در نظر گرفته می‌شوند را با $N_{cb}$ نمایش می‌دهیم \cite{spectral}.
\begin{equation}
N_{cp} = \{p | p \in P, \exists b \in B, NA(p,b) \ge \omega\}
\end{equation}

\begin{equation}
N_{cb} = \{b | b \in B, \exists p \in P, NA(p,b) \ge \omega\}
\end{equation}

\subsection{دقت}
دقت یک معیار ارزیابی مجموعه پروتئینی‌های شناسایی شده است که نشان می‌دهد چند مورد از مجموعه‌های پیش‌بینی شده الگوریتم به درستی انتخاب شده‌اند.
\begin{equation}
Precision = \frac{N_{cp}}{|P|}
\end{equation}

\subsection{بازیابی}
بازیابی دیگر معیار مورد توجه است که نشان می‌دهد چند مورد از مجموعه پروتئینی‌های مرجع توسط الگوریتم پیش‌بینی شده‌اند. به دیگر عبارت میزان پوشش الگوریتم از مجموعه پروتئینی‌های مرجع را اندازه‌گیری می‌کند.
\begin{equation}
Recall = \frac{N_{cb}}{|B|}
\end{equation}

\subsection{امتیاز F}
معیار امتیاز F میانگین همساز\footnote{\lr{Harmonic mean}} بین دو معیار دقت و بازیابی می‌باشد که به صورت مقابل محاسبه می‌شود:

\begin{equation}
F-score = \frac{2 \times Precision \times Recall}{Precision + Recall}
\end{equation}

\subsection{صحت}
معیار صحت به کمک دو معیار دیگر حساسیت خوشه‌بندی\footnote{\lr{Clustering-wise sensitivity (Sn)}}  و ارزش پیش‌بینی مثبت خوشه‌بندی\footnote{\lr{Clustering-wise positive predictive value (PPV)}} محاسبه می‌شود. با در نظر گرفتن $T_{i,j}$ به عنوان تعداد پروتئین‌هایی که هم در مجموعه پروتئینی $i$ ام و هم در مجموعه پروتئینی پیش‌بینی $j$ ام یافت می‌شوند و همچنین $N$ به عنوان تعداد پروتئین‌های مجموعه پروتئینی مرجع $i$، می‌توانیم $Sn$ و $PPV$ را به صورت مقابل تعریف کنیم:
$$
PPV = \frac{\sum_{j=1}^{|P|} max_{i=1}^{|B|}|T_{ij}|}{\sum_{j=1}^{|P|} \sum_{i=1}^{|B|} T_{ij}}
$$
$$
Sn = \frac{\sum_{i=1}^{|B|} max_{j=1}^{|P|}|T_{ij}|}{\sum_{i=1}^{|B|} N_i}
$$

از نظر مفهومی، معیار $PPV$ نشان‌دهنده نسبت مجموع بیشینه پروتئین‌های تطبیق‌یافته هر مجموعه پروتئینی پیش‌بینی‌شده با مجموعه‌های پروتئینی مرجع، به تعداد کل پروتئین‌های تطبیق‌یافته در مجموعه‌های پروتئینی پیش‌بینی‌شده است.
از سوی دیگر، معیار $Sn$ بیان‌کننده نسبت مجموع بیشینه پروتئین‌های تطبیق‌یافته هر مجموعه پروتئینی مرجع با مجموعه‌های پروتئینی پیش‌بینی‌شده، به تعداد کل پروتئین‌های موجود در مجموعه‌های پروتئینی مرجع است.
در نهایت به کمک این دو معیار می‌توان معیار صحت را به صورت مقابل محاسبه نمود:
$$
Acc = \sqrt{Sn. PPV}
$$